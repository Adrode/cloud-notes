#data_structures #algorithms

## Czym są built in i user defined data structures?
#built_in #user_defined
- Przykłady **built in**:
	- list
	- tuple
	- set
	- dictionary
- Przykłady **user defined**:
	- stack
	- queue
	- linked list
	- tree
	- graph

## Czym różni się static array od dynamic array od strings?
#static_array #dynamic_array #strings
- W Python standardowo nie ma czegoś takiego jak static array z poziomu pisania kodu
- **Static array** - tablica, która nie zmienia swojej długości
	- mam 5 elementów, usuwam jeden to zostaje mi ta sama długość, tylko jedno miejsce mam niezagospodarowane
	- mam 5 elementów i dodać kolejny element to tworzy problem, bo mam stałą długość tablicy
- **Dynamic array** - w Python jest to list, która dynamicznie dostosowuje długość tablicy (listy)
- **Strings** - w Python strings są **niemutowalne**. Można dobrać się do zawartości po indexach, ale nie da się dodać / usunąć / zamienić znaku na początku / końcu / środku

## Jak działa dynamic array (list)?
#dynamic_array 
- Jeśli mam listę, np. `[1, 5, 7, 9]` to przeważnie od razu zarezerwowane jest kilka dodatkowych miejsc, na zasadzie `[1, 5, 7, 9, 0, 0, 0, 0]`
	- Python robi to sam, pod maską
	- jeśli robimy `.append(11)` to nowa liczba zostanie "włożona" w najbliższe puste miejsce, czyli `[1, 5, 7, 9, 11, 0, 0, 0]`
	- dopiero w momencie, gdy puste miejsca zostaną zapełnione, lista jest kopiowana (na poziomie pamięci RAM) i w nowym miejscu w pamięci zostaje zapisania kopia z nowymi, dodatkowymi miejscami na kolejne elementy
		- wszystko to po to, aby operacje na list wychodziły średnio **\*O(1) amortized**
## Jak działa stack?
#stack 
- **Stack** - stos; porównać można do ułożonych na półce talerzy
	- jeśli mam 5 ponumerowanych talerzy, ułożonych jeden na drugim, to naturalną kolejnością wyciągania talerzy będzie nr. 5 > nr. 4 > nr. 3 > ... > nr. 1
		- czyli nie mogę dobrać się do talerza nr. 3, bez wcześniejszego dostępu do talerza nr. 4 i nr. 5 
	- Działania na list w formie struktury stack:
		- użycie na list **.append(*jakaś wartość*)**, doda po kolei wartości
		- kolejno użycie **.pop()** (bez podanego argumentu) usunie wartość z .pop zgodnie z zamysłem stack (czyli chronologicznie - od najświeższej do najstarszej)
		- ***list*\[-1]** również zwraca najświeższy element dodany do list
			- wszystkie te operacje są **O(1) w dynamic array (list)**

## Jak działa queue?
#queue
- **Queue** - odwrotność stack, czyli dostęp mamy od pierwszego elementu zbioru do kolejnych; jak kolejka ludzi w sklepie albo zasada FIFO
	- `['a', 'b', 'c']` - pierwszym elementem w kolejce jest `'a'`
	- **enqueue** - dodanie elementu do kolejki, czyli append (na końcu); **O(1)**
	- **deque** - usunięcie elementu z kolejki, ale zgodnie z priorytetem kolejki, czyli usunięcie 1szego elementu; **O(n)**, ponieważ usunięcie elementu wymusza przesunięcie pozostałych elementów do początku listy
		- **queue w formie listy nie ma sensu, ponieważ zbyt dużo operacji będzie O(n)**
- **Doubly linked list jest lepszą opcją kolejki, ponieważ łatwo można dodawać element do kolejki (enqueue; nowy tail) oraz usuwać elementy z kolejki (deque; nowy head)**
	- o linked list niżej

## Do czego służy i jak korzystać z deque?
#deque
- **Deque** - struktura danych, która odwzorowuje działanie queue; działa jak list, ale umożliwia szybkie operacje (dodawnie / usuwanie) danych z lewej strony (początek kolejki)
- ```python
  from collections import deque
  
  queue = deque()
  queue.append(1)
  queue.append(2)
  queue.append(3)
  print(queue) # [1, 2, 3]
  
  queue.popleft() # [2, 3]
  queue.appendleft(9) # [9, 2, 3]
  queue.pop() # [9, 2]
  ```
	- **`.appendleft(10)`** - dodanie elementu z lewej strony (enque; początek kolejki)
	- **`.popleft()`** - usunięcie elementu z lewej strony (deque; początek kolejki)
		- **oba są O(1), ponieważ działają na bazie doubly linked list**
## Czym jest linked list?
#linked_list
- **Linked list** - struktura powiązanych węzłów, gdzie każdy węzeł przechowuje wartość i wskaźniki do kolejnego elementu
	- listy są **węzłami (node)**, są połączone w łańcuchem i każda lista wskazuje na kolejną listę
- **Singly Linked List**:
	- każdy węzeł wskazuje na kolejny węzeł i łańcuch się zakańcza w momencie gdy ostatni węzeł nie ma kolejnej listy do powiązania, więc jego next będzie None
	- początkowy węzeł to **Head**, a zakończenie to po prostu zwrócenie None
	- ```python
	  class SinglyNode:
		  def __init__(self, value, next_node=None):
			  self.value = value
			  self.next_node = next_node
			  
		  def __str__(self):
			  return str(self.value)
			  
		Head = SinglyNode(1)
		A = SinglyNode(4)
		B = SinglyNode(7)
		C = SinglyNode(11)
		# powyższe to powiązania list w LinkedList
	  ```
		- wyświetlenie węzłów:
			- ```python
			  def display(head):
				  current = head
				  while current:
				  # zakończone w momencie gdy węzeł ksaże None, czyli koniec łańcucha
					  print(current)
					  current = current.next
					  # "przełączenie" na kolejny element łańcucha
			  ```
		- wyszukanie podanej wartości w łańcuchu:
			- ```python
			  def search(head, searched_value):
				  current = head
				  while current:
					  if current.value == searched_value:
						  return True
					  current = current.next
				  return False
			  ```
- **Doubly Linked List**:
	- podobnie jak w Singly Linked List, z tą różnicą że jest tutaj **Head** oraz **Tail** i łańcuch może przechodzić w jedną stronę (od Head, jak w Singly Linked) oraz w drugą (od Tail do początu)
	- tutaj również zakończeniem Łańcucha jest zwrócenie None, z tą różnicą, że w obu kierunkach na końcu jest None
	- ```python
	  class Node:
		  def __init__(self, value, next=None, prev=None):
			self.value = value
			self.next = next
			self.prev = prev
		
		  def __str__(self):
			return str(self.value)
		  
	  class DoublyLinkedList:
		  def __init__(self, head=None, tail=None):
			self.head = head
			self.tail = tail
		
		  def append(self, value):
			new_node = Node(value)
			if not self.head:
			  self.head = self.tail = new_node
			else:
			  self.tail.next = new_node
			  new_node.prev = self.tail
			  self.tail = new_node
		
		  def prepend(self, value):
			new_node = Node(value)
			if not self.head:
			  self.head = self.tail = new_node
			else:
			  self.head.prev = new_node
			  new_node.next = self.head
			  self.head = new_node
		
		  def del_from_beginning(self):
			if not self.head:
			  print("List is empty")
			  return
			print(f"Deleting from beginning: {self.head.value}")
			if self.head == self.tail:
			  self.head = self.tail = None
			else:
			  self.head = self.head.next
			  self.head.prev = None
		
		  def del_from_ending(self):
			if not self.head:
			  print("List is empty")
			  return
			print(f"Deleting from ending: {self.tail.value}")
			if self.head == self.tail:
			  self.head = self.tail = None
			else:
			  self.tail = self.tail.prev
			  self.tail.next = None
		
		  def display_forward(self):
			current = self.head
			elements = []
			while current:
			  elements.append(str(current.value))
			  current = current.next
			print(f"Forwards: {' <-> '.join(elements)}")
		
		  def display_backward(self):
			current = self.tail
			elements = []
			while current:
			  elements.append(str(current.value))
			  current = current.prev
			print(f"Backwards: {' <-> '.join(elements)}")
		
		dll = DoublyLinkedList()
		dll.append(1)
		dll.append(2)
		dll.prepend(3)
		dll.append(9)
		dll.append(11)
		dll.prepend(5)
		
		dll.display_forward()
		dll.del_from_beginning()
		dll.display_forward()
		dll.del_from_ending()
		dll.display_forward()
	  ```

### Czym są hash tables (functions, sets & maps)?
#hash
- **Hashable data structures** – to  typy danych, które **mogą być użyte jako klucze w hash mapach (np. w dict)**, czyli takie, które mają stałą wartość hasha (są niemutowalne).
    - **hashowalne (immutable)**: `tuple`, `str`, `int`
    - **niehashowalne (mutable)**: `dict`, `list`, `set`  
        (bo mogą się zmieniać, więc ich hash nie byłby stały)
- **Hash Function** – przekształca dane wejściowe (np. string, int, tuple) w liczbę całkowitą, czyli **hash value**.
    - Mamy np. **5 „bucketów”**, czyli pojemników (miejsc) na dane.
    - Funkcja hashująca może np.:
        - zamienić litery na liczby (a=1, b=2, c=3, itd.)
        - zsumować je - np. abc = 6
        - podzielić przez liczbę bucketów - 6 % 5 = 1
        - czyli wynik **1 to index bucketu**, do którego dane trafią
    - Dzięki temu możemy **od razu trafić do konkretnego miejsca w pamięci** – dlatego wyszukiwanie / dodawanie danych w hash mapie to średnio **O(1)**.
- **Kolizja** – występuje, gdy różne dane dają ten sam hash value i chcą trafić do tego samego bucketu.
    - Przykładowe rozwiązania:
        - **chaining** – w buckecie tworzy się lista (np. linked list) i kolejne elementy są po prostu dopisywane,
        - **open addressing** – szuka się kolejnego pustego bucketu.
    - Wtedy takie operacje mogą być **O(n)**, ale średnio i tak są **amortized O(1)**.
- **Hash Table** – to po prostu cała ta „tablica bucketów” w pamięci, w której dane są rozmieszczone według hash value.
- **Hash Map** – struktura danych oparta na hash table, która przechowuje **pary klucz–wartość** (np. dict).
    - **Wszystkie klucze są przepuszczane przez funkcję hashującą, co pozwala błyskawicznie znaleźć dane po kluczu.**
    - Dlatego operacje dodawania, usuwania i wyszukiwania są **O(1)** w czasie średnim.
- **Set** – działa bardzo podobnie do dict, tylko przechowuje **same klucze (bez wartości)**.
	- Wszystkie elementy w set są unikalne, bo każda wartość jest hashowana – więc nie może się powtórzyć.
	- Dzięki temu operacje typu in / add / remove też są \***O(1) amortized**.

## Do czego służy defaultdict?
#defaultdict
- ```python
  from collections import defaultdict
  
  default = defaultdict(int)
  default[5]
  ```
	- **`defaultdict`** - tworzy dict, z tym że w przypadku dodawania pary bez podania value / w przypadku wyszukiwania klucza którego nie ma w dict, nie wyrzuca `KeyError`, tylko tworzy klucz z bazową wartością (int - bazowa wartość 0, list - bazowa wartość \[])

## Czym jest rekurencja?
#recursion
- **Recursion** - wywołanie funkcji wewnątrz jej samej, np. ciąg Fibonacciego
- **Call Stack**:
	- funkcja dla ciągu Fibonacciego - `f(n) = f(n - 1) + f(n - 2)`
	- pod `n` podstawiam **pozycje ciągu**, a nie wartości
		- ![[Pasted image 20251106163752.png]]
		- czyli tutaj pod pozycją 4 jest liczba 3
	- jeśli pod `f(n)` podstawię liczbę 4 to algorytm działa tak:
		- `f(4)` potrzebuje (zgodnie z funkcją) `f(3) + f(2)`
			- w każdym punkcie tego stacka dany krok (tutaj `f(4)`) przekazuje do kolejnego kroku swój adres w pamięci, aby kolejny krok wiedział gdzie ma zwrócić rozwiązanie
			- **bardzo ważne! najpierw algorytm skupia się na rozwiązaniu pierwszej gałęzi drzewa, czyli `f(3)`**
				- jest to ważne przy tworzeniu call stacków, ponieważ zmiana pozycji w jakiej coś wykonujemy powoduje zmianę wykonywania się gałęzi drzewa (zdjęcie drzewa niżej), co ma wpływ na sposób wykonywania programu
		- `f(3)` potrzebuje `f(2) + f(1)`, więc najpierw rozwiązuje `f(2)`
		- `f(2)` potrzebuje `f(1) + f(0)`
			- zakładamy że w funkcji mamy założenia że dla `n == 0: return 0` i dla `n == 1: return 1`
		- mamy rozwiązanie `f(2) = 1 + 0`, czyli zwracamy do `f(2)` wartość `1`
		- teraz wracamy do `f(1)`, które zwraca `1`, więc `f(1)` zwraca wartość `1`
		- wracamy wyżej i wiemy, gdzie wiemy że w `f(3)` zwrócono dla `f(2) == 1`, więc przechodzimy do gałęzi `f(1)` - zwróci 1
		- `f(3)` zwróci `f(2) == 1` + `f(1) == 1`, czyli mamy wartość `2`
		- wracamy do `f(4)`, mamy rozwiązaną gałąź `f(3)`, gdzie zwróciliśmy 2, więc przechodzimy do gałęzi `f(2)` i rozwiązujemy ją analogicznie
			- wiemy już, że `f(2) == 1`
				- **tak właśnie działa rekurencja**
		- poniżej graficzna reprezentacja call stack (działa na powyższej zasadzie):
			- ![[Pasted image 20251106165101.png]]
		- poniżej graficzna reprezentacja drzewa (działa na powyższej zasadzie):
			- ![[Pasted image 20251106165147.png]]
	- w takim ciągu **time complexity** wynosi **O(2^n)**, ponieważ każdy kolejny krok to rozgałęzienie na kolejne 2 gałęzie
	- **space complexity** wynosi **O(n)**, tak jak było to rozpisane w graficznej reprezentacji stacka (space jest zależne od wejściowej liczby n)
- **Linked list, graphs (niżej), trees (niżej)** działają na zasadzie rekurencji

## Jak działa binary search?
#binary_search
- **Binary Search** to sposób wyszukiwania dopasowania w **posortowanej tablicy** (list / array)
	- przeszukiwanie poprzez **zwykłą iterację przez każdy element** daje time complexity **O(n)**; binary search **jest bardziej wydajne pod kątem liczby operacji do wykonania**
	- ![[Pasted image 20251109133243.png]]
		- szukamy liczby 3 w tablicy
			- `T == 3`
		- na początku i końcu listy ustawiamy **L** i **R**
		- wyznaczamy środkowy element listy **M**
			- tutaj mamy parzystą liczbę elementów tablicy, więc środkowe indexy to 2 i 3; w takim przypadku bierzemy mniejszy index poprzez:
				- **M = L + ((R - L) // 2)**
					- można też zrobić **M = (L + R) // 2**, ale przy dużych liczbach (np. R == 1000000), jest ryzyko tzw. **integer overflow**
			- teraz sprawdzamy, czy wartość pod środkowym indexem jest równa, mniejsza czy większa od szukanej liczby
				- `A[M] == T`, czyli `-2 == 3`, co daje `False`
			- wiemy teraz, że szukana liczba jest większa od liczby ze środkowym indexem, a że lista jest posortowana, to należy szukać w prawo
			- przesuwamy **L**, czyli początek zakresu przeszukiwania w pozycję **M + 1**, i teraz wskazujemy nowy środek przeszukiwanego zakresu w taki sam sposób jak wcześniej, czyli **M** ustawiamy na index 4
				- gdybyśmy przeszukiwali "lewą stronę" tablicy, to wtedy zmieniamy **R** na **M - 1**
			- `A[M] == 3`, a `T == 3`, więc mamy doapsowanie
				- **dzięki takiemu "połówkowemu" przeszukiwaniu tablicy w każdym kroku, time complexity wynosi O(log⌄2n (*log base 2 of n*))**, czyli jest to bardziej wydajne wyszukiwanie
				- **space complexity** wynosi **O(1)**, bo działamy ciągle na jednej tablicy
			- przeszukiwanie kończy się w momencie, gdy albo znajdziemy wyszukiwaną liczbę albo gdy **L** i **R** zamienią się miejscami (co jest trochę błędem logicznym i wskazuje, że szukanej liczby nie ma w tablicy)

## Jak działa binary tree?
#binary_tree
- **Binary tree** struktura drzewiasta, składająca się z **nodes**, w której każdy node może mieć rozgałęzienie na swoją lewą i prawą stronę
	- ![[Pasted image 20251110162700.png]]
		- **root** - każdy node, który ma rozgałęzienia (np. tutaj node 2 tworzy **subtree** i jest rootem dla tego subtree)
		- **leaf nodes** - "liście"; nodes, które nie mają dalszych rozgałęzień
		- **height** - wysokość drzewa; określana jest jako liczba poziomów / liczba "zejść w dół", czyli tutaj wysokość wynosi 3

## Jak działa binary search tree?
#binary_search_tree
- **Binary search tree** - "posortowane" drzewo, w którym wszystkie nodes po lewej stronie roota mają mniejszą wartość niż  root, a po prawej wszystkie wartości są większe niż wartość roota; **dotyczy to każdego roota w drzewie!**
	- ![[Pasted image 20251110171305.png]]
		- jeśli szukałbym wartości **9**, to od razu wiem, że po lewej stronie tej liczby nie będzie, bo **5 < 9**, czyli jeśli istnieje ta wartość, to jest z prawej strony
		- kolejny node ma wartość 8, a nadal szukamy 9, więc po lewej stronie node'a 8 na pewno tej liczby nie będzie, więc znowu szukamy na prawo
			- **time complexity będzie wynosił tutaj O(log⌄2n)**
## W jaki sposób można przeszukiwać binary trees?
#binary_tree_traversal
- Rodzaje **traversal / kolejności przeszukiwań po binary tree**:
	- **DFS (Depth First Search) Preorder** - najpierw wartość obecnego node, potem lewa strona potem prawa strona; taki schemat dla każdego node w drzewie, zaczynając od roota
		- (na bazie drzewa z obrazka wyżej) \[1, 2, 4, 5, 3, 10]
	- **DFS Inorder** - left > node > right
		- \[4, 2, 5, 1, 10, 3]
	- **DPF Postorder** - left > right > node
		- \[4, 5, 2, 10, 3, 1]
	- **BFS (Breadth First Search)** - inaczej **level order traversal**; przeszukiwanie po levelach, od lewej do prawej
		- \[1, 2, 3, 4, 5, 10]
	- **DFS korzysta ze stack (iteracja lub rekurencja po drzewie), BFS korzysta z queue**
		- **ona mają time complexity O(n) i space complexity O(n)**

## Jak działa heaps & priority queue?
#heaps #priority_queue
- **Priority queue** to koncepcyjna struktura, w której kolejka układa się na podstawie priorytetu; jest to struktura na podstawie **binary tree**
- **Heap** to implementacja priority queue
	- **Min Heap:**![[Pasted image 20251111103626.png]]
		- jest to struktura, w której rodzic ma zawsze mniejszą wartość niż którekolwiek z jego dzieci
			- **Max Heap** byłoby ułożone na zasadzie rodzic ma zawsze większą wartość niż którekolwiek z jego dzieci
		- **wartości w drzewie można reprezentować w formie tablicy; przykład:**
			- ![[Pasted image 20251111104045.png]] (te liczby nie są związane z heap wyżej)
				- **i** jest **indexem rodzica**, **2i + 1** to index lewego dziecka tego rodzica, **2i + 2** to index prawego dziecka tego rodzica; pod indexami będą wartości danego node'a
		- ![[Pasted image 20251111104559.png]]
			- **Heap pop** - **O(log2 n)**, pop usuwa element na szczycie (tutaj liczbę 2), a później przesuwa jego dzieci do góry na zasadzie: jeśli element z lewej strony jest mniejszy od elementu z prawej strony, to element z lewej strony jest promowany w kolejce (przesuwany do góry); w ten sposób odrzucamy prawą stronę, i przechodzimy do kolejnego node'a (na lewo od głównego roota) i tak samo sprawdzamy lewą i prawą stronę i po sprawdzeniu odrzucamy jedną stronę drzewa i promujemy kolejny node; **na tej podstawie jest to time O(log2 n)**
			- **Heap push - O(log2 n)**, czyli dodajemy element na dole drzewa i sprawdzamy czy rodzic jest większy od tego elementu (jeśli jest to przesuwamy do góry, dopóki nie spełnimy warunku, że rodzic < dziecko); **również time O(log2 n)**
			- **Heap peek - O(1)** - sprawdzenie najmniejszej liczby, czyli pierwszego elementu (w odwzorowaniu na tablicy będzie to arr\[0]), **czyli O(1)**
			- **Heap sort - O(n log2 n)** - czyli posortowanie wartości z drzewa do tablicy, w formie priority queue, czyli biorę najmniejszy element (główny root), robię append to array i potem promuję kolejny najmniejszy element (sprawdzanie lewej i prawej strony); czyli przechodzę po wszystkich elementach, ale w każdym kroku sprawdzam kolejną połowę drzewa, więc **time O(n log2 n)**
		- **Funkcja Heapify** - funkcja, która tworzy heap; procesuje poziomami od dołu (pomija leaf nodes) i podmienia wartości zgodnie z zasadą rodzic < dziecko (tzw. **sift down**); w ten sposób tworzy heap
			- **Time - O(n)**
			- **Space - O(1)**
	- **Max Heap** - działa analogicznie, z tą różnicą że rodzic > dziecko i główny root ma największą wartość

## Jakie są algorytmy sortowania?
#sorting_algorithms
- **Bubble sort** - metoda sortowania list
	- **Time: O(n^2), Space: O(1)**
	- ustawiamy iterator na **arr\[1]** i sprawdzamy czy poprzednia wartość jest większa - jeśli większa to robimy swap, jeśli mniejsza to zostawiamy jak jest
	- zwiększamy iterator, czyli teraz pozycja **arr\[2]** i sprawdzamy tak samo
		- i tak w kółko
	- zakańczamy pętlę dopiero w momencie, gdy kolejna iteracja po całej list nie spowodowała żadnej zamiany
```python
A = [9, 1, 15, 3, 2, 5, 6, -1, 3]

def bubble_sort(arr):
  n = len(arr)
  check = True
  while check:
    check = False
    for i in range(1, n):
      if arr[i] < arr[i-1]:
        check = True
        arr[i-1], arr[i] = arr[i], arr[i-1]

bubble_sort(A)
print(A)
		  ```

- **Insertion sort** - założenie jest takie, że początek list (może być to 1 element, może być więcej) **już jest posortowany** i sortujemy prawą, pozostałą część listy
	- **Time: O(n^2), Space: O(1)**
	- mamy 2 "wskaźniki", np. **'i'** i **'j'**; zaczynamy iterację od pierwszego elementu z nieposortowanej części tablicy (załóżmy index 1)
	- wskaźnik **'i'** wskazuje miejsce w którym zaczyna się nieposortowana tablica, a wskaźnik **'j'** służy do sprawdzania elementów od 'i' do początku tablicy i podmieniania kolejności
		- kiedy przesuwamy wskaźnik 'i' do przodu, to jednocześnie w to miejsce ustawiamy wskaźnik 'j' i znowu lecimy w dół sprawdzając, czy czegoś nie trzeba podmienić
```python
A = [-10, 1, 15, 3, 2, 5, 6, -1, 3]

def insertion_sort(arr):
  n = len(arr)
  for i in range(1, n):
    print(f"{i}: ")
    for j in range(i, 0, -1):
      if arr[j] < arr[j-1]:
        arr[j], arr[j-1] = arr[j-1], arr[j]
        print(f"{i}.{j}: {arr}")
      else:
        break

insertion_sort(A)
print(A)
```

- **Selection sort** - dwa "wskaźniki": **'i'** i **'j'**. 'i' jest ustawiony na pierwszym elemencie (arr\[0]) a 'j' iteruje po kolejnych elementach listy i gdy znajdzie mniejszy element, to zamienia je miejscami
	- **Time: O(n^2), Space: O(1)**
	- 'i' przesuwa się na kolejny element i iterujemy dalej, w ten sam sposób
```python
A = [9, -10, 1, 15, 3, 2, 5, 6, -1, 3]

def selection_sort(arr):
  n = len(arr)
  for i in range(n):
    min_element = i
    for j in range(i+1, n):
      if arr[j] < arr[min_element]:
        min_element = j
    arr[i], arr[min_element] = arr[min_element], arr[i]

selection_sort(A)
print(A)
```

- **Merge sort** - polega na "dzieleniu" w każdym kroku listy na pół (poprzez wyznaczony środkowy index na zasadzie (L+R) // 2); opiera się na rekurencji
	- **Time: O(n log2 n), Space: O(n) lub O(log2 n) - bardziej skomplikowana wersja**
	- dzielimy do momentu, aż nie dostaniemy wszędzie list z pojedynczymi elementami (długość == 1)
	- ![[Pasted image 20251112184827.png]]
	- teraz robimy **merge** po kolei par rozdzielonych list
		- dla każdej listy, które akurat merge'ujemy, ustawiamy "wskaźnik" **L** (dla lewej) i **R** (dla prawej)
		- porównujemy wartość pod indexem **L** z wartością **R**, wybieramy mniejszą wartość, robimy append tej wartości do nowej listy i inkrementujemy użyty w tym momencie wskaźnik (np. inkrementujemy **L**, jeśli właśnie w tej chwili braliśmy wartość spod **L** do nowej tablicy)
		- ![[Pasted image 20251112185346.png]]
		- ![[Pasted image 20251112185428.png]]
```python
A = [-5, 15, 3, -4, 2, 7, 18, -9, 0, 4, 3]

def merge_sort(arr):
  n = len(arr)

  if n == 1:
    return arr
  
  m = n // 2
  L = arr[:m]
  R = arr[m:]

  L = merge_sort(L)
  R = merge_sort(R)
  l, r = 0, 0
  L_len = len(L)
  R_len = len(R)

  sorted_arr = [0] * n
  i = 0

  while l < L_len and r < R_len:
    if L[l] < R[r]:
      sorted_arr[i] = L[l]
      l += 1
    else:
      sorted_arr[i] = R[r]
      r += 1
    i += 1

  while l < L_len:
    sorted_arr[i] = L[l]
    l += 1
    i += 1

  while r < R_len:
    sorted_arr[i] = R[r]
    r += 1
    i += 1

  return sorted_arr

print(A)
print(f"Merge sort: {merge_sort(A)}")
```


[Data Structures & Algorithms](https://www.youtube.com/watch?v=E2v9hBgG6gE&list=PLKYEe2WisBTFEr6laH5bR2J19j7sl5O8R&index=10)